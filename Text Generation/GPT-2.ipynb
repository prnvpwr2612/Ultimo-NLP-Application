{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:38.862963Z",
     "start_time": "2020-10-01T21:53:37.344493Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "tf.random.set_seed(42)  # for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:41.277751Z",
     "start_time": "2020-10-01T21:53:40.514853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel, OpenAIGPTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:53:46.161388Z",
     "start_time": "2020-10-01T21:53:42.619559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ppawa\\.cache\\huggingface\\hub\\models--openai-gpt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFOpenAIGPTLMHeadModel: ['h.3.attn.bias', 'h.9.attn.bias', 'h.6.attn.bias', 'h.11.attn.bias', 'h.2.attn.bias', 'h.5.attn.bias', 'h.0.attn.bias', 'h.7.attn.bias', 'h.10.attn.bias', 'h.4.attn.bias', 'h.8.attn.bias', 'h.1.attn.bias']\n",
      "- This IS expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFOpenAIGPTLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "gpttokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "gpt = TFOpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:54:50.685322Z",
     "start_time": "2020-10-01T21:54:38.240472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5846 9259  544  481]], shape=(1, 4), dtype=int32)\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "robotics is the only way to get to the surface. \" \n",
      " \" i'm not sure i understand. \" \n",
      " \" the first thing we have to do is find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to get to the surface. \" \n",
      " \" but how? \" \n",
      " \" we have to find a way to\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpttokenizer.encode('Robotics is the ', return_tensors='tf')\n",
    "print(input_ids)\n",
    "greedy_output = gpt.generate(input_ids, max_length=100)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(gpttokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:16.061027Z",
     "start_time": "2020-10-01T21:55:13.333259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ppawa\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "gpt2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", \n",
    "                                         pad_token_id=gpt2tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:23.517246Z",
     "start_time": "2020-10-01T21:55:17.435194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the vernacular of the future.\n",
      "\n",
      "The future is not a future where robots are going to be able to do anything. It's a future where robots are going to be able to do anything.\n",
      "\n",
      "The future is\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = gpt2tokenizer.encode('Robotics is the ', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = gpt2.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:35.102811Z",
     "start_time": "2020-10-01T21:55:27.887836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the vernacular of science fiction and fantasy. It's a genre that has been around for a long time. It's a genre that has been around for a long time. It's a genre that has been around for a long time\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # for reproducible results\n",
    "# BEAM SEARCH\n",
    "# activate beam search and early_stopping\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=51, \n",
    "    num_beams=20, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:55:51.708801Z",
     "start_time": "2020-10-01T21:55:41.049036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the vernacular term for a new kind of robot. It's a robot that can do a lot of things, but it can't do them all. It can do things that other robots can't.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 3\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=3, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:56:18.662519Z",
     "start_time": "2020-10-01T21:56:06.291288Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "\n",
      "0: Robotics is the vernacular term for the use of robots to solve problems.\n",
      "\n",
      "Robotics has been around for a long time. It was first used in the early 20th century as a means of transportation, but it was also used\n",
      "\n",
      "1: Robotics is the vernacular term for the use of robots to solve problems.\n",
      "\n",
      "Robotics has been around for a long time. It was first used in the early 20th century as a means of solving problems. It has been used\n",
      "\n",
      "2: Robotics is the vernacular term for the use of robots to solve problems.\n",
      "\n",
      "Robotics has been around for a long time. It was first used in the early 20th century as a means of solving problems in the field of robotics\n"
     ]
    }
   ],
   "source": [
    "# Returning multiple beams\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_outputs = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=7, \n",
    "    no_repeat_ngram_size=3, \n",
    "    num_return_sequences=3,  \n",
    "    early_stopping=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"\\n{}: {}\".format(i, \n",
    "                        gpt2tokenizer.decode(beam_output, \n",
    "                                             skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:56:37.968897Z",
     "start_time": "2020-10-01T21:56:31.887781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "Robotics is the izarro version of your traditional training that is more challenging than regular cycling. While the basic skills vary a bit from country to country (or vice versa), one core thing we've learned, with very clear guidelines, has always\n"
     ]
    }
   ],
   "source": [
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    do_sample=True, \n",
    "    top_k=25,\n",
    "    temperature=2.000\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:57:44.012864Z",
     "start_time": "2020-10-01T21:57:18.453191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "In the dark of the night, there was a urn lying with the broken blade of a long sword at its shoulder.\n",
      "\n",
      "\"And if anyone's wrong, we will find out in five days if this sword is left there.\"\n",
      "\n",
      "The Dragon Spirit of Fire said with a smile.\n",
      "\n",
      "\"No, it is still at the end of the world. The sword of the Dragon Spirit was lost yesterday.\"\n",
      "\n",
      "With a look at it, the Dragon Spirit started to speak with such a cool feeling that the feeling of the heart of the Dragon could become a sound.\n",
      "\n",
      "\"The reason for the sword of the Dragon is that it was left by Anjou when the Dragon was a child. It was because his father's sword, his life partner's sword. So he would be lost while holding the Dragon Spirit. It is a pity this Dragon Spirit was left here, he could go right now to the forest, where he could be able to heal. Also,\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt2tokenizer.encode('In the dark of the night, there was a ', return_tensors='tf')\n",
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    \n",
    "    do_sample=True, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:57:53.136325Z",
     "start_time": "2020-10-01T21:57:44.063001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ppawa\\.cache\\huggingface\\hub\\models--gpt2-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Another sample with a larger model\n",
    "gpt2tok_l = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "gpt2_l = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", \n",
    "                                         pad_token_id=gpt2tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T21:59:04.169629Z",
     "start_time": "2020-10-01T21:57:53.178268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "--------------------------------------------------\n",
      "In the dark of the night, there was a urn containing the remains of his son, a child so dear to him, so much loved and honoured, so often called for in their father's presence. It sat in the hall of a large house in the country; and from a window on the first floor of a larger and better-built one, he could see a crowd of people; and they were whispering among themselves as if for a signal; and he could see them in the darkness at the door of his own house, and he could hear them, too, as they passed the window, and he could hear the sound of a door opening and closing. When the light went on, and the people came out to look at the urn, his face shone like the sun in the evening, for his son's body lay in the urn beside him, and he was smiling, and his eyes seemed to be full of life and light.\n",
      "\n",
      "Then came a servant,\n"
     ]
    }
   ],
   "source": [
    "input_ids = gpt2tok_l.encode('In the dark of the night, there was a ', return_tensors='tf')\n",
    "# Top-K sampling\n",
    "tf.random.set_seed(42)  # for reproducible results\n",
    "beam_output = gpt2_l.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 50 * '-')\n",
    "print(gpt2tok_l.decode(beam_output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "366px",
    "left": "1112px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
