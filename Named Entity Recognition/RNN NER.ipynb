{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1556d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ebbc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data\\conllpp_train.txt\n",
      "Found and verified data\\conllpp_dev.txt\n",
      "Found and verified data\\conllpp_test.txt\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
    "dir_name = 'data'\n",
    "def download_data(url, filename, download_dir, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "      \n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "    \n",
    "    # Check the file size\n",
    "    statinfo = os.stat(filepath)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filepath)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "# Filepaths to train/valid/test data\n",
    "train_filepath = download_data(url, 'conllpp_train.txt', dir_name, 3283420)\n",
    "dev_filepath = download_data(url, 'conllpp_dev.txt', dir_name, 827443)\n",
    "test_filepath = download_data(url, 'conllpp_test.txt', dir_name, 748737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f5f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Reading data ...\n",
      "\tDone\n",
      "Train size: 14041\n",
      "Valid size: 3250\n",
      "Test size: 3452\n",
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: LONDON 1996-08-30\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "\n",
      "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of sentences (each sentence a string), \n",
    "    and list of ner labels for each string\n",
    "    '''\n",
    "\n",
    "    print(\"Reading data ...\")\n",
    "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
    "    sentences, ner_labels = [], [] \n",
    "    \n",
    "    # Open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
    "        \n",
    "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                is_sos = False\n",
    "            # Otherwise keep capturing tokens and labels\n",
    "            else:\n",
    "                is_sos = True\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            \n",
    "            # When we reach the end / or reach the beginning of next\n",
    "            # add the data to the master lists, flush the temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    print('\\tDone')\n",
    "    return sentences, ner_labels\n",
    "\n",
    "# Train data\n",
    "train_sentences, train_labels = read_data(train_filepath) \n",
    "# Validation data\n",
    "valid_sentences, valid_labels = read_data(dev_filepath) \n",
    "# Test data\n",
    "test_sentences, test_labels = read_data(test_filepath) \n",
    "\n",
    "# Print some stats\n",
    "print(f\"Train size: {len(train_labels)}\")\n",
    "print(f\"Valid size: {len(valid_labels)}\")\n",
    "print(f\"Test size: {len(test_labels)}\")\n",
    "\n",
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
    "    print(f\"Sentence: {v_sent}\")\n",
    "    print(f\"Labels: {v_labels}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0602956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label counts\n",
      "O         169578\n",
      "B-LOC       7140\n",
      "B-PER       6600\n",
      "B-ORG       6321\n",
      "I-PER       4528\n",
      "I-ORG       3704\n",
      "B-MISC      3438\n",
      "I-LOC       1157\n",
      "I-MISC      1155\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation data label counts\n",
      "O         42759\n",
      "B-PER      1842\n",
      "B-LOC      1837\n",
      "B-ORG      1341\n",
      "I-PER      1307\n",
      "B-MISC      922\n",
      "I-ORG       751\n",
      "I-MISC      346\n",
      "I-LOC       257\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test data label counts\n",
      "O         38143\n",
      "B-ORG      1714\n",
      "B-LOC      1645\n",
      "B-PER      1617\n",
      "I-PER      1161\n",
      "I-ORG       881\n",
      "B-MISC      722\n",
      "I-LOC       259\n",
      "I-MISC      252\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Print the value count for each label\n",
    "print(\"Training data label counts\")\n",
    "print(pd.Series(chain(*train_labels)).value_counts())\n",
    "\n",
    "print(\"\\nValidation data label counts\")\n",
    "print(pd.Series(chain(*valid_labels)).value_counts())\n",
    "\n",
    "print(\"\\nTest data label counts\")\n",
    "print(pd.Series(chain(*test_labels)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a793c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14041.000000\n",
       "mean        14.501887\n",
       "std         11.602756\n",
       "min          1.000000\n",
       "5%           2.000000\n",
       "50%         10.000000\n",
       "95%         37.000000\n",
       "max        113.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68a579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "def get_label_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class label -> class ID mapping\n",
    "    labels_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    print(f\"labels_map: {labels_map}\")\n",
    "    return labels_map\n",
    "\n",
    "def get_padded_int_labels(labels, labels_map, max_seq_length, return_mask=True):\n",
    "\n",
    "    # Convert string labels to integers \n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and \n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                int_labels, maxlen=max_seq_length, padding='post', truncating='post', value=-1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    \n",
    "    else:\n",
    "        padded_labels = np.array(ner_pad_sequence_func(int_labels, value=labels_map['O']))\n",
    "        return padded_labels\n",
    "\n",
    "    \n",
    "labels_map = get_label_id_map(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e7c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of sequences\n",
    "max_seq_length = 40\n",
    "\n",
    "# Size of token embeddings\n",
    "embedding_size = 64\n",
    "\n",
    "# Number of hidden units in the RNN layer\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "# Number of output nodes in the last layer\n",
    "n_classes = 9\n",
    "\n",
    "# Number of samples in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633ee8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]\n",
      " [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1]]\n",
      "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "padded_train_labels, train_mask = get_padded_int_labels(\n",
    "    train_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "padded_valid_labels, valid_mask = get_padded_int_labels(\n",
    "    valid_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "padded_test_labels, test_mask  = get_padded_int_labels(\n",
    "    test_labels, labels_map, max_seq_length, return_mask=True\n",
    ")\n",
    "\n",
    "# Print some labels IDs\n",
    "print(padded_train_labels[:2])\n",
    "print(train_mask[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d798dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "With default arguments\n",
      "\n",
      "Data: \n",
      "[[ 9  4  6  2  3  8  7]\n",
      " [ 2  3  5 10  0  0  0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']\n",
      "--------------------------------------------------\n",
      "\n",
      "With limited vocabulary\n",
      "\n",
      "Data: \n",
      "[[1 4 1 2 3 1 1]\n",
      " [2 3 1 1 0 0 0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went']\n",
      "--------------------------------------------------\n",
      "\n",
      "With preprocessing disabled\n",
      "\n",
      "Data: \n",
      "[[12  2  4  5  7  6 10]\n",
      " [ 9 11  3  8  0  0  0]]\n",
      "Vocabulary: ['', '[UNK]', 'went', 'was', 'to', 'the', 'on', 'market', 'empty.', 'The', 'Sunday', 'Market', 'I']\n",
      "--------------------------------------------------\n",
      "\n",
      "With a maximum sequence length\n",
      "\n",
      "Data: \n",
      "[[ 9  4  6  2]\n",
      " [ 2  3  5 10]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "toy_corpus = [\"I went to the market on Sunday\", \"The Market was empty.\"]\n",
    "toy_vectorization_layer = TextVectorization()\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "toy_vectorized_output = toy_vectorization_layer(toy_corpus)\n",
    "toy_vocabulary = toy_vectorization_layer.get_vocabulary()\n",
    "\n",
    "print(\"With default arguments\\n\")\n",
    "print(f\"Data: \\n{toy_vectorized_output}\")\n",
    "print(f\"Vocabulary: {toy_vocabulary}\")\n",
    "print('-'*50)\n",
    "\n",
    "toy_vectorization_layer = TextVectorization(max_tokens=5)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith limited vocabulary\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)\n",
    "\n",
    "toy_vectorization_layer = TextVectorization(standardize=None)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith preprocessing disabled\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)\n",
    "\n",
    "toy_vectorization_layer = TextVectorization(output_sequence_length=4)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith a maximum sequence length\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d7f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        max_tokens=vocabulary_size, standardize=None,        \n",
    "        output_sequence_length=max_seq_length, \n",
    "    )\n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "# Input layer\n",
    "word_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Text vectorize layer\n",
    "vectorize_layer, n_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "vectorized_out = vectorize_layer(word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "embedding_layer = layers.Embedding(input_dim=n_vocab, output_dim=embedding_size, mask_zero=True)(vectorized_out)\n",
    "\n",
    "# Define a simple RNN layer, it returns an output at each position\n",
    "rnn_layer = layers.SimpleRNN(\n",
    "    units=rnn_hidden_size, return_sequences=True\n",
    ")\n",
    "\n",
    "rnn_out = rnn_layer(embedding_layer)\n",
    "\n",
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out)\n",
    "\n",
    "model = tf.keras.Model(inputs=word_input, outputs=dense_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c303ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_accuracy(y_true, y_pred):\n",
    "    \n",
    "    y_true = tf.cast(tf.reshape(y_true, [-1]), 'int32')\n",
    "    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-1), [-1]), 'int32')\n",
    "    \n",
    "    sorted_y_true = tf.sort(y_true)\n",
    "    sorted_inds = tf.argsort(y_true)\n",
    "    \n",
    "    sorted_y_pred = tf.gather(y_pred, sorted_inds)\n",
    "    \n",
    "    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, sorted_y_pred), 'int32')\n",
    "    \n",
    "    correct_for_each_label = tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), 'float32') + 1\n",
    "    all_for_each_label = tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), sorted_y_true), 'float32') + 1\n",
    "    \n",
    "    mean_accuracy = tf.reduce_mean(correct_for_each_label/all_for_each_label)\n",
    "    \n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e60dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,512,000</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ simple_rnn          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)         │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │ simple_rnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │  \u001b[38;5;34m1,512,000\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ simple_rnn          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │      \u001b[38;5;34m8,256\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mSimpleRNN\u001b[0m)         │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m9\u001b[0m)     │        \u001b[38;5;34m585\u001b[0m │ simple_rnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,520,841</span> (5.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,520,841\u001b[0m (5.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,520,841</span> (5.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,520,841\u001b[0m (5.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ab9018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "Class weights: {1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.17500000000000002, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298186, 7: 1.0}\n"
     ]
    }
   ],
   "source": [
    "def get_class_weights(train_labels):\n",
    "    \n",
    "    label_count_ser = pd.Series(chain(*train_labels)).value_counts()\n",
    "    label_count_ser = label_count_ser.sum()/label_count_ser\n",
    "    label_count_ser /= label_count_ser.max()\n",
    "    \n",
    "    label_id_map = get_label_id_map(train_labels)\n",
    "    label_count_ser.index = label_count_ser.index.map(label_id_map)\n",
    "    return label_count_ser.to_dict()\n",
    "\n",
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "\n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(f\"Class weights: {train_class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "742d4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=512,\n",
    "    output_sequence_length=max_seq_length,\n",
    "    output_mode='int'\n",
    ")\n",
    "vectorize_layer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee0425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b5988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 40) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, 40), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m valid_vectorized_sentences \u001b[38;5;241m=\u001b[39m vectorize_layer(np\u001b[38;5;241m.\u001b[39marray(valid_sentences))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Training the model with vectorized sentences\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_vectorized_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_train_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_vectorized_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_valid_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:542\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 542\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be 1. Received: inputs.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m         )\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    549\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TextVectorization.call().\n\n\u001b[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 40) with rank=2\u001b[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(None, 40), dtype=string)"
     ]
    }
   ],
   "source": [
    "# Apply vectorization to convert sentences into token IDs\n",
    "train_vectorized_sentences = vectorize_layer(train_sentences)\n",
    "valid_vectorized_sentences = vectorize_layer(np.array(valid_sentences))\n",
    "\n",
    "# Training the model with vectorized sentences\n",
    "model.fit(\n",
    "    train_vectorized_sentences, padded_train_labels,\n",
    "    sample_weight=train_sample_weights,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(valid_vectorized_sentences, padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c57948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 3s 32ms/step - loss: 0.1287 - macro_accuracy: 0.7804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1287258118391037, 0.7803568840026855]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "217ce397",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: str4192",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m visual_test_sentences \u001b[38;5;241m=\u001b[39m test_sentences[:n_samples]\n\u001b[0;32m      3\u001b[0m visual_test_labels \u001b[38;5;241m=\u001b[39m padded_test_labels[:n_samples]\n\u001b[1;32m----> 5\u001b[0m visual_test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_test_sentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m visual_test_pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(visual_test_predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m rev_labels_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(labels_map\u001b[38;5;241m.\u001b[39mvalues(), labels_map\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optree\\ops.py:594\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    592\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    593\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: str4192"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "visual_test_sentences = test_sentences[:n_samples]\n",
    "visual_test_labels = padded_test_labels[:n_samples]\n",
    "\n",
    "visual_test_predictions = model.predict(np.array(visual_test_sentences))\n",
    "visual_test_pred_labels = np.argmax(visual_test_predictions, axis=-1)\n",
    "\n",
    "rev_labels_map = dict(zip(labels_map.values(), labels_map.keys()))\n",
    "for i, (sentence, sent_labels, sent_preds) in enumerate(zip(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    \n",
    "    n_tokens = len(sentence.split())\n",
    "    print(\"Sample:\\t\",\"\\t\".join(sentence.split()))\n",
    "    print(\"True:\\t\",\"\\t\".join([rev_labels_map[i] for i in sent_labels[:n_tokens]]))\n",
    "    print(\"Pred:\\t\",\"\\t\".join([rev_labels_map[i] for i in sent_preds[:n_tokens]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "553b1654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23623.000000\n",
       "mean         6.832705\n",
       "std          2.749288\n",
       "min          1.000000\n",
       "5%           3.000000\n",
       "50%          7.000000\n",
       "95%         12.000000\n",
       "max         61.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ser = pd.Series(pd.Series(train_sentences).str.split().explode().unique())\n",
    "vocab_ser.str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c07fc70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence: [[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']]]\n",
      "Vectorized output: [[[2 2 2 2]\n",
      "  [4 4 0 0]\n",
      "  [6 0 0 0]]\n",
      "\n",
      " [[5 0 0 0]\n",
      "  [3 3 3 0]\n",
      "  [0 0 0 0]]]\n",
      "Vocabulary: ['', '[UNK]', 'a', 'e', 'b', 'd', 'c']\n"
     ]
    }
   ],
   "source": [
    "def split_char(token):\n",
    "    \"\"\" Instead of splitting word by word, split each char\"\"\"\n",
    "    return tf.strings.bytes_split(token)\n",
    "\n",
    "\n",
    "# Define a vectorization layer that splits chars\n",
    "vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=split_char,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length):\n",
    "    \"\"\" Pads each sequence to a maximum length \"\"\"\n",
    "    proc_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        if len(tokens) >= max_seq_length:\n",
    "            proc_sentences.append([[t] for t in tokens[:max_seq_length]])\n",
    "        else:\n",
    "            proc_sentences.append([[t] for t in tokens+['']*(max_seq_length-len(tokens))])\n",
    "            \n",
    "    return proc_sentences\n",
    "\n",
    "# Define sample data\n",
    "data = ['aaaa bb c', 'd eee']\n",
    "# Pad sequences\n",
    "tokenized_sentences = prepare_corpus_for_char_embeddings([d.split() for d in data], 3)\n",
    "print(f\"Padded sequence: {tokenized_sentences}\")\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "vectorization_layer.adapt(tokenized_sentences)\n",
    "\n",
    "# Print data\n",
    "print(f\"Vectorized output: {vectorization_layer(tokenized_sentences)}\")\n",
    "print(f\"Vocabulary: {vectorization_layer.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625215c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd']]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _split_char(token):\n",
    "    return tf.strings.bytes_split(token)\n",
    "\n",
    "_split_char(tf.constant(['abcd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da8abc46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 1), dtype=string, sparse=False, name=keras_tensor>',)\n  • kwargs={'mask': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 75\u001b[0m\n\u001b[0;32m     68\u001b[0m char_vectorize_layer, n_char_vocab \u001b[38;5;241m=\u001b[39m get_fitted_char_vectorization_layer(train_sentences, max_seq_length, max_token_length)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Vectorized output of each token\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Need a [batch size, seq len, 1]\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# strings.split() returns a RaggedTensor. It needs to be converted to a Tensor. Otherwise the following error will be raised\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# InvalidArgumentError:  assertion failed: [the given axis (axis = 2) is not squeezable!]\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#\t [[node model/text_vectorization_1/RaggedSqueeze/Assert/Assert (defined at <ipython-input-26-a2f55ee22434>:17) ]] [Op:__inference_train_function_72435]\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m tokenized_word_input \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m char_vectorized_out \u001b[38;5;241m=\u001b[39m char_vectorize_layer(tokenized_word_input)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Produces a [batch size, seq length, token_length, emb size]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\lambda_layer.py:95\u001b[0m, in \u001b[0;36mLambda.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mshape, output_spec)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe could not automatically infer the shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe Lambda\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms output. Please specify the `output_shape` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument for this Lambda layer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         )\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape(input_shape)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 1), dtype=string, sparse=False, name=keras_tensor>',)\n  • kwargs={'mask': 'None'}"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "max_seq_length = 40\n",
    "max_token_length = 12\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        max_tokens=vocabulary_size, standardize=None,        \n",
    "        output_sequence_length=max_seq_length, \n",
    "    )\n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "def get_fitted_char_vectorization_layer(corpus, max_seq_length, max_token_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    def _split_char(token):\n",
    "        return tf.strings.bytes_split(token)\n",
    "\n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=_split_char,\n",
    "        output_sequence_length=max_token_length, \n",
    "    )\n",
    "\n",
    "    tokenized_sentences = [sent.split() for sent in corpus]\n",
    "    padded_tokenized_sentences = prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length)\n",
    "    \n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(padded_tokenized_sentences)\n",
    "    \n",
    "    # Get the vocabulary size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "\n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "# Input layer (tokens)\n",
    "word_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# --------------------- Token based Text Vectorizer + Embeddings ----------------------- #\n",
    "# Text vectorize layer (token)\n",
    "token_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "token_vectorized_out = token_vectorize_layer(word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "token_embedding_out = layers.Embedding(input_dim=n_token_vocab, output_dim=64, mask_zero=True)(token_vectorized_out)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------#\n",
    "\n",
    "# -------------- Char based Text Vectorizer + Convolutional embeddings ----------------- #\n",
    "\n",
    "# Text vectorize layer (char)\n",
    "char_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_layer(train_sentences, max_seq_length, max_token_length)\n",
    "\n",
    "# Vectorized output of each token\n",
    "# Need a [batch size, seq len, 1]\n",
    "# strings.split() returns a RaggedTensor. It needs to be converted to a Tensor. Otherwise the following error will be raised\n",
    "# InvalidArgumentError:  assertion failed: [the given axis (axis = 2) is not squeezable!]\n",
    "#\t [[node model/text_vectorization_1/RaggedSqueeze/Assert/Assert (defined at <ipython-input-26-a2f55ee22434>:17) ]] [Op:__inference_train_function_72435]\n",
    "tokenized_word_input = layers.Lambda(\n",
    "    lambda x: tf.strings.split(x).to_tensor(default_value='', shape=[None, max_seq_length, 1])\n",
    ")(word_input)\n",
    "char_vectorized_out = char_vectorize_layer(tokenized_word_input)\n",
    "\n",
    "\n",
    "# Produces a [batch size, seq length, token_length, emb size]\n",
    "char_embedding_layer = layers.Embedding(input_dim=n_char_vocab, output_dim=32, mask_zero=True)(char_vectorized_out)\n",
    "\n",
    "# A 1D convolutional layer that will generate token embeddings by shifting a convolutional kernel over \n",
    "# the sequence of chars in each token (padded)\n",
    "char_token_output = layers.Conv1D(filters=1, kernel_size=5, strides=1, padding='same', activation='relu')(char_embedding_layer)\n",
    "# There is an additional dimension of size 1 (out channel dimension) that we need to remove\n",
    "char_token_output = layers.Lambda(lambda x: x[:, :, :, 0])(char_token_output)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------#\n",
    "\n",
    "# Concatenate the token and char embeddings\n",
    "concat_embedding_out = layers.Concatenate()([token_embedding_out, char_token_output])\n",
    "\n",
    "# Define a simple RNN layer, it returns an output at each position\n",
    "rnn_layer_1 = layers.SimpleRNN(\n",
    "    units=64, activation='tanh', use_bias=True, return_sequences=True\n",
    ")\n",
    "\n",
    "rnn_out_1 = rnn_layer_1(concat_embedding_out)\n",
    "\n",
    "# Defines the final prediction layer\n",
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out_1)\n",
    "\n",
    "# Defines the model\n",
    "char_token_embedding_rnn = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    " \n",
    "# Define a macro accuracy measure\n",
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n",
    "\n",
    "# Compile the model with a loss optimizer and metrics\n",
    "char_token_embedding_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "# Summary of the model\n",
    "char_token_embedding_rnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58320e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7adc3bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "Class weights: {1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.17500000000000002, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298186, 7: 1.0}\n",
      "Epoch 1/3\n",
      "220/220 [==============================] - 64s 231ms/step - loss: 0.0288 - macro_accuracy: 0.5550 - val_loss: 0.4320 - val_macro_accuracy: 0.7073\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 48s 220ms/step - loss: 0.0087 - macro_accuracy: 0.8833 - val_loss: 0.1971 - val_macro_accuracy: 0.7977\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 49s 221ms/step - loss: 0.0030 - macro_accuracy: 0.9576 - val_loss: 0.1046 - val_macro_accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26cc2cea208>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(f\"Class weights: {train_class_weights}\")\n",
    "\n",
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "char_token_embedding_rnn.fit(\n",
    "    train_sentences, padded_train_labels,\n",
    "    sample_weight=train_sample_weights,\n",
    "    batch_size=64,\n",
    "    epochs=3, \n",
    "    validation_data=(np.array(valid_sentences), padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fda96a",
   "metadata": {},
   "source": [
    "## Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd09441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 4s 38ms/step - loss: 0.1151 - macro_accuracy: 0.7597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1150645762681961, 0.7596610188484192]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_token_embedding_rnn.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b1bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
