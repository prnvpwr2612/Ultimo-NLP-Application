{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:49:21.185576Z",
     "start_time": "2020-10-01T07:49:19.290613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.9.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_root = './gmb-2.2.0/data/'\n",
    "\n",
    "fnames = []\n",
    "for root, dirs, files in os.walk(data_root):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".tags\"):\n",
    "            fnames.append(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!mkdir ner    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    " \n",
    "ner_tags = collections.Counter()\n",
    "iob_tags = collections.Counter()\n",
    "\n",
    "def strip_ner_subcat(tag):\n",
    "    # NER tags are of form {cat}-{subcat}\n",
    "    # eg tim-dow. We only want first part\n",
    "    return tag.split(\"-\")[0]\n",
    "\n",
    "\n",
    "def iob_format(ners):\n",
    "    # converts IO tags into BIO format\n",
    "    # input is a sequence of IO NER tokens\n",
    "    # convert this: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    iob_tokens = []\n",
    "    for idx, token in enumerate(ners):\n",
    "        if token != 'O':  # !other\n",
    "            if idx == 0:\n",
    "                token = \"B-\" + token #start of sentence\n",
    "            elif ners[idx-1] == token:\n",
    "                token = \"I-\" + token  # continues\n",
    "            else:\n",
    "                token = \"B-\" + token\n",
    "        iob_tokens.append(token)\n",
    "        iob_tags[token] += 1\n",
    "    return iob_tokens  \n",
    "\n",
    "total_sentences = 0\n",
    "outfiles = []\n",
    "for idx, file in enumerate(fnames):\n",
    "    with open(file, 'rb') as content:\n",
    "        data = content.read().decode('utf-8').strip()\n",
    "        sentences = data.split(\"\\n\\n\")\n",
    "        print(idx, file, len(sentences))\n",
    "        total_sentences += len(sentences)\n",
    "        \n",
    "        with open(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file), 'w') as outfile:\n",
    "            outfiles.append(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file))\n",
    "            writer = csv.writer(outfile)\n",
    "            \n",
    "            for sentence in sentences: \n",
    "                toks = sentence.split('\\n')\n",
    "                words, pos, ner = [], [], []\n",
    "                \n",
    "                for tok in toks:\n",
    "                    t = tok.split(\"\\t\")\n",
    "                    words.append(t[0])\n",
    "                    pos.append(t[1])\n",
    "                    ner_tags[t[3]] += 1\n",
    "                    ner.append(strip_ner_subcat(t[3]))\n",
    "                writer.writerow([\" \".join(words), \n",
    "                                 \" \".join(iob_format(ner)), \n",
    "                                 \" \".join(pos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of sentences: \", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n",
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "print(ner_tags)\n",
    "print(iob_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m labels, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miob_tags\u001b[38;5;241m.\u001b[39mitems())\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels, values = zip(*iob_tags.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m indexes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(indexes, values)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(indexes, labels, rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(len(labels))\n",
    "\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels, rotation='vertical')\n",
    "plt.margins(0.01)\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:49:58.146943Z",
     "start_time": "2020-10-01T07:49:40.500192Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# could use `outfiles` param as well\u001b[39;00m\n\u001b[0;32m      5\u001b[0m files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ner/*.tags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m data_pd \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\Users\\ppawa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# could use `outfiles` param as well\n",
    "files = glob.glob(\"./ner/*.tags\")\n",
    "\n",
    "data_pd = pd.concat([pd.read_csv(f, header=None, \n",
    "                                 names=[\"text\", \"label\", \"pos\"]) \n",
    "                for f in files], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:49:58.222022Z",
     "start_time": "2020-10-01T07:49:58.200917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62010 entries, 0 to 62009\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    62010 non-null  object\n",
      " 1   label   62010 non-null  object\n",
      " 2   pos     62010 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:49:58.279243Z",
     "start_time": "2020-10-01T07:49:58.275476Z"
    }
   },
   "outputs": [],
   "source": [
    "### Keras tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "text_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n",
    "                     split=' ', oov_token='<OOV>')\n",
    "\n",
    "pos_tok = Tokenizer(filters='\\t\\n', lower=False,\n",
    "                    split=' ', oov_token='<OOV>')\n",
    "\n",
    "ner_tok = Tokenizer(filters='\\t\\n', lower=False,\n",
    "                    split=' ', oov_token='<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:01.206804Z",
     "start_time": "2020-10-01T07:49:58.331382Z"
    }
   },
   "outputs": [],
   "source": [
    "text_tok.fit_on_texts(data_pd['text'])\n",
    "pos_tok.fit_on_texts(data_pd['pos'])\n",
    "ner_tok.fit_on_texts(data_pd['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:01.433120Z",
     "start_time": "2020-10-01T07:50:01.273733Z"
    }
   },
   "outputs": [],
   "source": [
    "ner_config = ner_tok.get_config()\n",
    "text_config = text_tok.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:01.599616Z",
     "start_time": "2020-10-01T07:50:01.596871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_words': None, 'filters': '\\t\\n', 'lower': False, 'split': ' ', 'char_level': False, 'oov_token': '<OOV>', 'document_count': 62010, 'word_counts': '{\"O\": 1146068, \"B-org\": 26195, \"B-per\": 21984, \"I-per\": 22270, \"B-tim\": 26296, \"B-geo\": 48876, \"I-geo\": 9512, \"I-org\": 21899, \"I-tim\": 8493, \"B-gpe\": 20436, \"I-gpe\": 244, \"B-nat\": 238, \"B-eve\": 391, \"I-eve\": 318, \"B-art\": 503, \"I-art\": 364, \"I-nat\": 62}', 'word_docs': '{\"O\": 61999, \"B-org\": 20478, \"B-per\": 17499, \"I-per\": 13805, \"B-tim\": 22345, \"B-geo\": 31660, \"I-geo\": 7738, \"I-org\": 11011, \"I-tim\": 5526, \"B-gpe\": 16565, \"I-gpe\": 224, \"B-nat\": 211, \"B-eve\": 361, \"I-eve\": 201, \"B-art\": 425, \"I-art\": 207, \"I-nat\": 50}', 'index_docs': '{\"2\": 61999, \"5\": 20478, \"7\": 17499, \"6\": 13805, \"4\": 22345, \"3\": 31660, \"10\": 7738, \"8\": 11011, \"11\": 5526, \"9\": 16565, \"16\": 224, \"17\": 211, \"13\": 361, \"15\": 201, \"12\": 425, \"14\": 207, \"18\": 50}', 'index_word': '{\"1\": \"<OOV>\", \"2\": \"O\", \"3\": \"B-geo\", \"4\": \"B-tim\", \"5\": \"B-org\", \"6\": \"I-per\", \"7\": \"B-per\", \"8\": \"I-org\", \"9\": \"B-gpe\", \"10\": \"I-geo\", \"11\": \"I-tim\", \"12\": \"B-art\", \"13\": \"B-eve\", \"14\": \"I-art\", \"15\": \"I-eve\", \"16\": \"I-gpe\", \"17\": \"B-nat\", \"18\": \"I-nat\"}', 'word_index': '{\"<OOV>\": 1, \"O\": 2, \"B-geo\": 3, \"B-tim\": 4, \"B-org\": 5, \"I-per\": 6, \"B-per\": 7, \"I-org\": 8, \"B-gpe\": 9, \"I-geo\": 10, \"I-tim\": 11, \"B-art\": 12, \"B-eve\": 13, \"I-art\": 14, \"I-eve\": 15, \"I-gpe\": 16, \"B-nat\": 17, \"I-nat\": 18}'}\n"
     ]
    }
   ],
   "source": [
    "print(ner_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:01.916426Z",
     "start_time": "2020-10-01T07:50:01.763217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in vocab: 39422\n",
      "Unique NER tags in vocab: 18\n"
     ]
    }
   ],
   "source": [
    "text_vocab = eval(text_config['index_word'])\n",
    "ner_vocab = eval(ner_config['index_word'])\n",
    "\n",
    "print(\"Unique words in vocab:\", len(text_vocab))\n",
    "print(\"Unique NER tags in vocab:\", len(ner_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:03.727420Z",
     "start_time": "2020-10-01T07:50:02.125340Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tok = text_tok.texts_to_sequences(data_pd['text'])\n",
    "y_tok = ner_tok.texts_to_sequences(data_pd['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:05.020042Z",
     "start_time": "2020-10-01T07:50:05.013495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Faure Gnassingbe said in a speech carried by state media Friday he will remain interim president until elections take place .'] Faure Gnassingbe said in a speech carried by state media Friday he will remain interim president until elections take place .\n",
      "['B-per I-per O O O O O O O O B-tim O O O O O O O O O O'] B-per I-per O O O O O O O O B-tim O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "print(text_tok.sequences_to_texts([x_tok[1]]), data_pd['text'][1])\n",
    "print(ner_tok.sequences_to_texts([y_tok[1]]), data_pd['label'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:07.320085Z",
     "start_time": "2020-10-01T07:50:06.299910Z"
    }
   },
   "outputs": [],
   "source": [
    "# now, pad seqences to a maximum length\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "x_pad = sequence.pad_sequences(x_tok, padding='post',\n",
    "                              maxlen=max_len)\n",
    "y_pad = sequence.pad_sequences(y_tok, padding='post',\n",
    "                              maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:08.617395Z",
     "start_time": "2020-10-01T07:50:08.613831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62010, 50) (62010, 50)\n"
     ]
    }
   ],
   "source": [
    "print(x_pad.shape, y_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:09.909511Z",
     "start_time": "2020-10-01T07:50:09.905495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Faure Gnassingbe said in a speech carried by state media Friday he will remain interim president until elections take place . <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tok.sequences_to_texts([x_pad[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:11.205921Z",
     "start_time": "2020-10-01T07:50:11.201932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-per I-per O O O O O O O O B-tim O O O O O O O O O O <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tok.sequences_to_texts([y_pad[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:12.589474Z",
     "start_time": "2020-10-01T07:50:12.485211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62010, 50, 19)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(ner_vocab)+1\n",
    "\n",
    "Y = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training the BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:13.897362Z",
     "start_time": "2020-10-01T07:50:13.891121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary \n",
    "vocab_size = len(text_vocab) + 1 \n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=90\n",
    "\n",
    "# num of NER classes\n",
    "num_classes = len(ner_vocab)+1\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "\n",
    "dropout=0.2\n",
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n",
    "  model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    Bidirectional(LSTM(units=rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           dropout=dropout,  \n",
    "                           kernel_initializer=tf.keras.initializers.he_normal())),\n",
    "    TimeDistributed(Dense(rnn_units, activation='relu')),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "  ])\n",
    "\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:16.756095Z",
     "start_time": "2020-10-01T07:50:15.207599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (90, None, 64)            2523072   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (90, None, 200)           132000    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 100)         20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 19)          1919      \n",
      "=================================================================\n",
      "Total params: 2,677,091\n",
      "Trainable params: 2,677,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_bilstm(\n",
    "                        vocab_size = vocab_size,\n",
    "                        embedding_dim=embedding_dim,\n",
    "                        rnn_units=rnn_units,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        classes=num_classes)\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:18.075113Z",
     "start_time": "2020-10-01T07:50:18.072829Z"
    }
   },
   "outputs": [],
   "source": [
    "X = x_pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:19.380299Z",
     "start_time": "2020-10-01T07:50:19.376566Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training and testing splits\n",
    "total_sentences = 62010\n",
    "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
    "X_train = X[BATCH_SIZE*test_size:]\n",
    "Y_train = Y[BATCH_SIZE*test_size:]\n",
    "\n",
    "X_test = X[0:BATCH_SIZE*test_size]\n",
    "Y_test = Y[0:BATCH_SIZE*test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:20.720818Z",
     "start_time": "2020-10-01T07:50:20.717698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49590, 50) (49590, 50, 19)\n",
      "(12420, 50) (12420, 50, 19)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:26:32.190739Z",
     "start_time": "2020-10-01T07:22:45.053249Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "551/551 [==============================] - 60s 110ms/step - loss: 0.1755 - accuracy: 0.9105\n",
      "Epoch 2/15\n",
      "551/551 [==============================] - 64s 117ms/step - loss: 0.0457 - accuracy: 0.9689\n",
      "Epoch 3/15\n",
      "551/551 [==============================] - 62s 112ms/step - loss: 0.0342 - accuracy: 0.9758\n",
      "Epoch 4/15\n",
      "551/551 [==============================] - 61s 111ms/step - loss: 0.0286 - accuracy: 0.9791\n",
      "Epoch 5/15\n",
      "551/551 [==============================] - 59s 107ms/step - loss: 0.0247 - accuracy: 0.9815\n",
      "Epoch 6/15\n",
      "551/551 [==============================] - 60s 108ms/step - loss: 0.0214 - accuracy: 0.9840\n",
      "Epoch 7/15\n",
      "551/551 [==============================] - 59s 108ms/step - loss: 0.0187 - accuracy: 0.9859\n",
      "Epoch 8/15\n",
      "551/551 [==============================] - 61s 110ms/step - loss: 0.0163 - accuracy: 0.9876\n",
      "Epoch 9/15\n",
      "551/551 [==============================] - 61s 111ms/step - loss: 0.0142 - accuracy: 0.9891\n",
      "Epoch 10/15\n",
      "551/551 [==============================] - 62s 113ms/step - loss: 0.0125 - accuracy: 0.9904\n",
      "Epoch 11/15\n",
      "551/551 [==============================] - 62s 112ms/step - loss: 0.0109 - accuracy: 0.9916\n",
      "Epoch 12/15\n",
      "551/551 [==============================] - 62s 112ms/step - loss: 0.0098 - accuracy: 0.9924\n",
      "Epoch 13/15\n",
      "551/551 [==============================] - 64s 116ms/step - loss: 0.0086 - accuracy: 0.9933\n",
      "Epoch 14/15\n",
      "551/551 [==============================] - 60s 110ms/step - loss: 0.0079 - accuracy: 0.9939\n",
      "Epoch 15/15\n",
      "551/551 [==============================] - 60s 109ms/step - loss: 0.0071 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbce70a83d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:30:22.389110Z",
     "start_time": "2020-10-01T07:30:19.360580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 4s 28ms/step - loss: 0.0932 - accuracy: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09319353848695755, 0.9619451761245728]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size in eval\n",
    "model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:31:09.265580Z",
     "start_time": "2020-10-01T07:31:06.508102Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:31:10.421772Z",
     "start_time": "2020-10-01T07:31:10.411459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Faure Gnassingbe said in a speech carried by state media Friday he will remain interim president until elections take place . <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tok.sequences_to_texts([X_test[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:31:11.710701Z",
     "start_time": "2020-10-01T07:31:11.703990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-per I-per O O O O O O O O B-tim O O O O O O O O O O <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tok.sequences_to_texts([y_pad[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:31:12.881266Z",
     "start_time": "2020-10-01T07:31:12.838115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([12420, 50])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tf.argmax(y_pred, -1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:31:16.076195Z",
     "start_time": "2020-10-01T07:31:16.071500Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pnp = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:31:17.478032Z",
     "start_time": "2020-10-01T07:31:17.471271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-per I-per O O O O O O O O B-tim O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tok.sequences_to_texts([y_pnp[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/aa/3732e4371470d593e8f47ecc6c6277ee8d54053631de3773a5ddf8011ff2/tensorflow_addons-0.9.1-cp37-cp37m-manylinux2010_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 4.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading https://files.pythonhosted.org/packages/06/37/d236aec27f8a8eed66f1a17116eb51684528cf8005a6883f879fe2e842ae/typeguard-2.7.1-py3-none-any.whl\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.9.1 typeguard-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:39.809176Z",
     "start_time": "2020-10-01T07:50:39.802402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "tfa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:43.446024Z",
     "start_time": "2020-10-01T07:50:43.433025Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class CRFLayer(Layer):\n",
    "  \"\"\"\n",
    "  Computes the log likelihood during training\n",
    "  Performs Viterbi decoding during prediction\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               label_size,\n",
    "               mask_id=0,\n",
    "               trans_params=None,\n",
    "               name='crf',\n",
    "               **kwargs):\n",
    "    super(CRFLayer, self).__init__(name=name, **kwargs)\n",
    "    self.label_size = label_size\n",
    "    self.mask_id = mask_id\n",
    "    self.transition_params = None\n",
    "    \n",
    "    if trans_params is None:  # not reloading pretrained params\n",
    "        self.transition_params = tf.Variable(tf.random.uniform(shape=(label_size, label_size)),\n",
    "                                         trainable=False)\n",
    "    else:\n",
    "        self.transition_params = trans_params\n",
    "\n",
    "  def get_seq_lengths(self, matrix):\n",
    "    # matrix is of shape (batch_size, max_seq_len)\n",
    "    mask = tf.not_equal(matrix, self.mask_id)\n",
    "    seq_lengths = tf.math.reduce_sum(\n",
    "                                    tf.cast(mask, dtype=tf.int32), \n",
    "                                    axis=-1)\n",
    "    return seq_lengths\n",
    "\n",
    "  def call(self, inputs, seq_lengths, training=None):\n",
    "    if training is None:\n",
    "        training = K.learning_phase()\n",
    "    \n",
    "    # during training, this layer just returns the logits\n",
    "    if training:\n",
    "        return inputs\n",
    "    \n",
    "    # viterbi decode logic to return proper \n",
    "    # results at inference\n",
    "    _, max_seq_len, _ = inputs.shape\n",
    "    seqlens = seq_lengths\n",
    "    paths = []\n",
    "    for logit, text_len in zip(inputs, seqlens):\n",
    "        viterbi_path, _ = tfa.text.viterbi_decode(logit[:text_len], \n",
    "                                              self.transition_params)\n",
    "        paths.append(self.pad_viterbi(viterbi_path, max_seq_len))\n",
    "\n",
    "    return tf.convert_to_tensor(paths) \n",
    "  \n",
    "  def pad_viterbi(self, viterbi, max_seq_len):\n",
    "    if len(viterbi) < max_seq_len:\n",
    "        viterbi = viterbi + [self.mask_id] * (max_seq_len - len(viterbi))\n",
    "    return viterbi\n",
    "\n",
    "  def get_proper_labels(self, y_true):\n",
    "    shape = y_true.shape\n",
    "    if len(shape) > 2:\n",
    "        return tf.argmax(y_true, -1, output_type=tf.int32)\n",
    "    return y_true\n",
    "        \n",
    "  def loss(self, y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(self.get_proper_labels(y_true), y_pred.dtype)\n",
    "\n",
    "    seq_lengths = self.get_seq_lengths(y_true)\n",
    "    log_likelihoods, self.transition_params = tfa.text.crf_log_likelihood(y_pred, \n",
    "                                                                y_true, seq_lengths)\n",
    "\n",
    "    # save transition params\n",
    "    self.transition_params = tf.Variable(self.transition_params, trainable=False)\n",
    "    # calc loss\n",
    "    loss = - tf.reduce_mean(log_likelihoods)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:45.110068Z",
     "start_time": "2020-10-01T07:50:45.100257Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class NerModel(tf.keras.Model):\n",
    "    def __init__(self, hidden_num, vocab_size, label_size, embedding_size,\n",
    "                name='BilstmCrfModel', **kwargs):\n",
    "        super(NerModel, self).__init__(name=name, **kwargs)\n",
    "        self.num_hidden = hidden_num\n",
    "        self.vocab_size = vocab_size\n",
    "        self.label_size = label_size\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embedding_size, \n",
    "                                   mask_zero=True, name=\"embedding\")\n",
    "        self.biLSTM =Bidirectional(LSTM(hidden_num, return_sequences=True), name=\"bilstm\")\n",
    "        self.dense = TimeDistributed(tf.keras.layers.Dense(label_size), name=\"dense\")\n",
    "        self.crf = CRFLayer(self.label_size, name=\"crf\")\n",
    "\n",
    "    def call(self, text, labels=None, training=None):\n",
    "        seq_lengths = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), \n",
    "                                               dtype=tf.int32), axis=-1) \n",
    "        \n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        inputs = self.embedding(text)\n",
    "        bilstm = self.biLSTM(inputs)\n",
    "        logits = self.dense(bilstm)\n",
    "        outputs = self.crf(logits, seq_lengths, training)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:51.092008Z",
     "start_time": "2020-10-01T07:50:50.958876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(text_vocab)+1 # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=90\n",
    "\n",
    "# num of NER classes\n",
    "num_classes = len(ner_vocab)+1\n",
    "\n",
    "blc_model = NerModel(rnn_units, vocab_size, num_classes, embedding_dim, dynamic=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:50:52.678017Z",
     "start_time": "2020-10-01T07:50:52.441222Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training and testing splits\n",
    "total_sentences = 62010\n",
    "test_size = round(total_sentences / BATCH_SIZE * 0.2)\n",
    "X_train = x_pad[BATCH_SIZE*test_size:]\n",
    "Y_train = Y[BATCH_SIZE*test_size:]\n",
    "\n",
    "X_test = x_pad[0:BATCH_SIZE*test_size]\n",
    "Y_test = Y[0:BATCH_SIZE*test_size]\n",
    "Y_train_int = tf.cast(Y_train, dtype=tf.int32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train_int))\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:59:23.496763Z",
     "start_time": "2020-10-01T07:50:55.018160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(73.376625, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(31.27511, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(23.989388, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(20.623888, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(18.11321, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(16.339197, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(15.013313, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(13.896758, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(13.030142, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(12.252562, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(11.550811, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(10.937271, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(10.925785, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(10.372887, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(9.863149, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(9.417708, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(9.000656, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(8.632322, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(8.304649, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(7.9951677, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(7.7288017, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(7.4774537, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(7.2400618, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(7.0250773, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(7.021103, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(6.820888, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(6.63081, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(6.458931, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(6.2927856, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(6.1378417, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(5.993965, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(5.855747, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(5.73302, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(5.611326, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(5.49427, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(5.384849, shape=(), dtype=float32)\n",
      "Start of epoch 3\n",
      "step 0: mean loss = tf.Tensor(5.3828626, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(5.2789645, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(5.1792026, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(5.0867624, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(4.997003, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(4.910585, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(4.8285556, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(4.7484317, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(4.675704, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(4.603433, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(4.5329976, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(4.4654164, shape=(), dtype=float32)\n",
      "Start of epoch 4\n",
      "step 0: mean loss = tf.Tensor(4.4642744, shape=(), dtype=float32)\n",
      "step 50: mean loss = tf.Tensor(4.399745, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(4.3364983, shape=(), dtype=float32)\n",
      "step 150: mean loss = tf.Tensor(4.2779975, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(4.2206774, shape=(), dtype=float32)\n",
      "step 250: mean loss = tf.Tensor(4.164841, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(4.1104784, shape=(), dtype=float32)\n",
      "step 350: mean loss = tf.Tensor(4.0570602, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(4.008206, shape=(), dtype=float32)\n",
      "step 450: mean loss = tf.Tensor(3.9590786, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(3.911234, shape=(), dtype=float32)\n",
      "step 550: mean loss = tf.Tensor(3.8649943, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (text_batch, labels_batch) in enumerate(train_dataset):\n",
    "        labels_max = tf.argmax(labels_batch, -1, output_type=tf.int32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = blc_model(text_batch, training=True)\n",
    "            loss = blc_model.crf.loss(labels_max, logits)\n",
    "\n",
    "            grads = tape.gradient(loss, blc_model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, blc_model.trainable_weights))\n",
    "            \n",
    "            loss_metric(loss)\n",
    "        if step % 50 == 0:\n",
    "          print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:59:24.998073Z",
     "start_time": "2020-10-01T07:59:24.938185Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_test_int = tf.cast(Y_test, dtype=tf.int32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test_int))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:48.287877Z",
     "start_time": "2020-10-01T08:01:46.634623Z"
    }
   },
   "outputs": [],
   "source": [
    "out = blc_model.predict(test_dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:49.754585Z",
     "start_time": "2020-10-01T08:01:49.749208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  2  2  2  2  3 10  2  7  6  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0] tf.Tensor(\n",
      "[ 3  2  2  2  2  3 10  2  7  6  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0], shape=(50,), dtype=int64)\n",
      "[2 2 2 5 8 2 2 7 6 2 2 2 2 2 2 2 2 2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0] tf.Tensor(\n",
      "[2 2 5 8 8 2 2 7 5 2 2 2 2 2 2 2 2 2 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# check the outputs\n",
    "print(out[1], tf.argmax(Y_test[1], -1))\n",
    "print(out[2], tf.argmax(Y_test[2], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:51.219944Z",
     "start_time": "2020-10-01T08:01:51.216208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Writing in The Washington Post newspaper , Mr. Ushakov also said it is inadmissible to move in the direction of demonizing Russia . <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tok.sequences_to_texts([X_test[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:52.674636Z",
     "start_time": "2020-10-01T08:01:52.668425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:  ['O O B-org I-org I-org O O B-per B-org O O O O O O O O O O O O B-geo O <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']\n",
      "Prediction:  ['O O O B-org I-org O O B-per I-per O O O O O O O O O O O B-geo B-geo O <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground Truth: \", ner_tok.sequences_to_texts([tf.argmax(Y_test[2], -1).numpy()]))\n",
    "print(\"Prediction: \", ner_tok.sequences_to_texts([out[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:54.139935Z",
     "start_time": "2020-10-01T08:01:54.135563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-geo O O O O B-geo I-geo O B-per I-per O O O O O O O O O O O O O O O O O O O O O O O <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']\n",
      "['B-geo O O O O B-geo I-geo O B-per I-per O O O O O O O O O O O O O O O O O O O O O O O <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']\n"
     ]
    }
   ],
   "source": [
    "print(ner_tok.sequences_to_texts([tf.argmax(Y_test[1], -1).numpy()]))\n",
    "print(ner_tok.sequences_to_texts([out[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:55.608400Z",
     "start_time": "2020-10-01T08:01:55.602471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BilstmCrfModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2523072   \n",
      "_________________________________________________________________\n",
      "bilstm (Bidirectional)       multiple                  132000    \n",
      "_________________________________________________________________\n",
      "dense (TimeDistributed)      multiple                  3819      \n",
      "_________________________________________________________________\n",
      "crf (CRFLayer)               multiple                  361       \n",
      "=================================================================\n",
      "Total params: 2,659,252\n",
      "Trainable params: 2,658,891\n",
      "Non-trainable params: 361\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:57.092264Z",
     "start_time": "2020-10-01T08:01:57.086114Z"
    }
   },
   "outputs": [],
   "source": [
    "def np_precision(pred, true):\n",
    "    # expect numpy arrays\n",
    "    assert pred.shape == true.shape\n",
    "    assert len(pred.shape) == 2\n",
    "    mask_pred = np.ma.masked_equal(pred, 0)\n",
    "    mask_true = np.ma.masked_equal(true, 0)\n",
    "    acc = np.equal(mask_pred, mask_true)\n",
    "    return np.mean(acc.compressed().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T08:01:57.100678Z",
     "start_time": "2020-10-01T08:01:57.093756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636105860113422"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_precision(out, tf.argmax(Y_test[:BATCH_SIZE], -1).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "647px",
    "left": "1200px",
    "right": "20px",
    "top": "120px",
    "width": "390px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
